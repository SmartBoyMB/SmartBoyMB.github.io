[{"title":"long tail Relation","date":"2020-11-06T04:45:19.754Z","url":"/SmartBoyMB.github.io/2020/11/06/longtailrelation/","tags":[["知识图谱","/SmartBoyMB.github.io/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"],["概念","/SmartBoyMB.github.io/tags/%E6%A6%82%E5%BF%B5/"]],"categories":[["学习","/SmartBoyMB.github.io/categories/%E5%AD%A6%E4%B9%A0/"]],"content":"“long tail”的含义 根据维基百科，长尾（The Long Tail）这一概念是由“连线”杂志主编Chris Anderson在2004年十月的“长尾” 一文中最早提出，用来描述诸如亚马逊和Netflix之类网站的商业和经济模式。 “长尾”实际上是统计学中Power Laws和帕累托（Pareto）分布特征的一个口语化表达。 在任何一组东西中，最重要的只占其中一小部分，约20%，其余80%尽管是多数，却是次要的，因此又称二八定律。 举例来说，我们常用的汉字实际上不多，但因出现频次高，所以这些为数不多的汉字占据了上图广大的红区；绝大部分的汉字难得一用，它们就属于那长长的黄尾。 “long tail”的理解 假设一个坐标轴以每个实体所具有的三元组数为横轴，以具有该三元组数实体的计数为纵轴，由于只有少量的实体具有较多数量的三元组，大多数实体具有的三元组数量较少，所以坐标轴会呈现一种随着三元组数增加，实体计数越来越少的现象，并且三元组数少的实体占比较大，呈现长尾的形态，这些实体被称为长尾实体。也可以用我们常说的二八定律来解释。 "},{"title":"少样本知识图谱推理","date":"2020-11-04T12:52:30.853Z","url":"/SmartBoyMB.github.io/2020/11/04/xiaoyangbenzhishitupu/","tags":[["知识图谱","/SmartBoyMB.github.io/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"],["论文学习","/SmartBoyMB.github.io/tags/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"]],"categories":[["学习","/SmartBoyMB.github.io/categories/%E5%AD%A6%E4%B9%A0/"]],"content":"摘要 早先的对于知识图谱（KGs）的研究中对于每种关系通常要求大量的训练用例。然而，我们发现长尾关系（long-tail relations）在知识图谱（KGs）中更加的普遍并且这些新添加的关系通常没有使用太多的已知三元组训练。 在这项工作中，我们的目标是在一个具有挑战性的环境下预测新的事实，其中只有一个培训实例可用。 在这项工作中，我们的目标是在一个具有挑战性的环境下预测新的事实，其中只有一个培训实例可用。 我们提出了一种次关系学习框架，它利用嵌入模型提取的知识，并通过考虑学习的嵌入和一跳图结构来学习匹配度量。我们的模型比现有的嵌入模型产生了相当大的性能改进，并且在处理新增加的关系时也不需要重新训练嵌入模型。 研究介绍 Large-scale knowledge graphs(Suchanek et al.,2007; Vrandeˇci´c and Kr¨otzsch, 2014; Bollacker et al., 2008; Auer et al., 2007; Carlson et al., 2010)将每一条信息表示为实体之间的二进制关系，通常以三元组的形式表示。 （主语，谓语，宾语）。 这种结构化知识对于许多下游应用程序(如问答和语义Web)是必不可少的。 Despite KGs’ large scale, they are known to be highly incomplete (Min et al., 2013). To automat\u0002ically complete KGs, extensive research efforts (Nickel et al., 2011; Bordes et al., 2013; Yang et al., 2014; Trouillon et al., 2016; Lao and Cohen, 2010; Neelakantan et al., 2015; Xiong et al.,2017; Das et al., 2017; Chen et al., 2018) 已经建立了关系学习模型，可以通过从现有的学习来推断缺失的三元组。 这些方法探索三元组或路径模式的统计信息，以推断现有关系的新事实；并在各种公共数据集上取得了相当大的性能。 然而，这些数据集(例如:以前的模型使用的FB15k，WN18)大多只涵盖KGS中的公共关系。对于更实际的场景，我们认为所需的KG完成模型应该处理KG的两个关键属性。 KG关系的很大一部分实际上是长尾关系。换句话说，它们的实例很少。但从直觉上看，一个关系的训练三元组越少，KG完成技术就越有用。因此，模型能够以有限的三元组完成关系是至关重要的。 为了捕捉最新的知识，现实世界的KGS通常在任何特定的时刻都是动态的和进化的。适应新关系的能力对于当前模型也是有限的。 与以往的方法相比，我们提出了一个只依赖于实体嵌入和局部图结构的模型。我们的模型旨在学习一个匹配的度量，可以用来发现更多相似的三元组，给定一个参考三元组。可学习度量模型是基于一个置换不变网络，它有效地编码实体的one-hop邻居，也是一个允许多步匹配的递归神经网络。一旦训练，模型将能够对任何关系进行预测，而现有的方法通常需要微调以适应新的关系。通过两个新构建的数据集，我们证明了我们的模型可以在一次链路预测任务上实现对各种嵌入模型的一致改进。 总之，我们的贡献有三个方面： 我们是第一个考虑链路预测任务中的长尾关系，并将问题描述为少镜头关系学习。 我们提出了一个有效的关系数据一次性学习框架，比各种基于嵌入的方法获得了更好的性能。 我们还为one-shot知识的任务提供了两个新构造的数据集。 相关工作 嵌入关系学习模型（Embedding Models for Relational Learning） 参考的一些模型 RESCAL (Nickel et al., 2011)算法：基于张量分解的算法。 Antoine Bordes, Nicolas Usunier, Alberto GarciaDuran, Jason Weston, and Oksana Yakhnenko.2013. Translating embeddings for modeling multirelational data.：提出了一维矢量空间中的关系建模方法。 DistMult (Yang et al., 2014)，ComplEx (Trouillon et al., 2016) and ConvE (Dettmers et al., 2017)： 已经被提出了。这些基于嵌入的模型通常为所有关系和实体假设足够的训练实例，并且不注意这些稀疏符号。 Baoxu Shi and Tim Weninger. 2017. Open-world knowledge graph completion.：已经被建议通过利用文本描述来处理看不见的实体。 与这些方法相比，我们的模型处理长尾关系或新添加的关系，并侧重于一次关系学习，没有任何外部信息，如实体或关系的文本描述。 少样本学习（Few-Shot Learning） 最近基于深度学习的少镜头学习方法分为两大类： 基于度量的方法：它试图从一组训练任务中学习可推广的度量和相应的匹配函数。这类方法大多采用深层孪生网络（Siamese network）中提出的通用匹配框架。 基于元学习的方法：它的目的是学习模型参数的优化（通过输出参数更新或直接预测模型参数），给出了几个例子的梯度。一个例子是基于LSTM的元学习器，它学习随机梯度的每个维度的步长。 除了上述类别之外，还有一些其他风格的少镜头学习算法，例如：贝叶斯程序归纳，它将概念表示为简单的程序，在贝叶斯准则下最好地解释观察到的例子。 以往的少样本学习研究主要集中在视觉和模仿学习领域。在语言领域，也提出了一种基于多指标的文本分类方法。据我们所知，这项工作是关于知识图的少样本学习的第一项研究。 背景 问题构想 知识图G表示为三元组，当和分别为实体序列和关系序列。 知识图完成的任务是预测在两个实例之间看不见的关系：或者在给予头部实体和查询关系的情况下预测尾部实体：。由于我们的目的是为新增加的或现有的长尾关系推断看不见的事实，我们将重点放在后一种情况上。与以前的工作相比，通常假设查询关系有足够的三元组可用于培训，本工作研究的情况是只有一个培训三元组可用。 更具体的说，对于真正的尾部实体的排列是要高于其他候选实体，对于唯一给定的一个三元组例子。候选集是使用实体类型约束构造的。同样值得注意的是，当我们预测关系的新事实时，我们只考虑一组封闭的实体。换句话说，测试期间没有未见的实体。对于在测试过程中可能出现新实体的开放世界设置，通常需要外部信息，如关于这些实体的文本描述，我们将此留给今后的工作。 一次学习设定 我们工作的目的是学习一个度量，可以用来预测新的事实与一个热门的例子。遵循标准的一次性学习设置。我们假设访问一组培训任务。 在我们的问题中，每个训练任务对应一个KG关系，并有自己的培训/测试三元组：。此任务集通常表示为元训练集，。 在模拟评估时的一次预测，只有一个三元组在每个。由的测试三元组和对于每个查询真实标签尾部实体。以及相应的尾部实体候选人，其中每个是中一个实体。因此，通过排列由测试查询和在中的标记三元组得到的候选人序列，这个度量模型可以被测试在这个序列。 我们表示一个任意排序损失函数表示为，代表的是我们度量模型的参数。这个损失函数表示了度量模型在元组如何，同时观察来自的仅一次数据。这个训练度量模型的目标，换句话说，元训练的目的，因此成为 是从元训练集中采样的，并且表明为的元组数。 一旦训练，我们就可以使用模型对新关系进行预测，这被称为文献中的元测试步骤。这些元测试关系是在元训练中看不见的，换句话说，。每个元测试关系也有自己的一次训练数据和测试数据，定义方式与元训练相同。这些元测试关系形成了一个元测试集。 此外，我们在训练中省略了一个关系子集，元验证集。 由于一次学习的假设，元测试关系没有像传统的机器学习设置那样的验证集。否则，度量模型实际上会在元测试期间看到多个一次标记数据，因此会违反了一次假设。 最终，我们假定一个方法可以访问背景知识图谱(background knowledge graph)，这是一个删除了,,和所有联系的的子集 模型 这一节，描述了一个提出的相似度量学习模型，以及相应的用于训练模型的损失函数。 提出模型的核心是相似度量函数。因此对于任意的查阅关系，只要有一个已知的事实(h_0,r,t_0)，这个模型就可以跟根据每个和之间的匹配分数才预测测试三元组的可能性。上述匹配功能的实现涉及两个子问题： 实体对的表示。 两个实体对表示之间的比较函数。 我们整体模型，如图1所示： Neighbor encoder 目的是利用局部图结构更好地表示实体。这样，模型可以利用KG为实体对中的每个实体提供的更多信息，增强每个实体在知识图中的本地连接的表示。 显示建模的结构模式，如路径，通常有利于关系预测。鉴于此，我们建议使用邻居编码器将图形结构纳入我们的度量学习模型。 为了从结构信息中受益，同时保持效率，以方便地扩展到现实世界的大规模KGS，我们的邻居编码器只考虑实体的本地连接，即，一跳邻居(one-hop neighbors)。 对于给定的实体，其本地连接形成一组（关系、实体）元组。如图1中（a）所示，对于实体Leonardo da Vinci，这样的元组之一是（职业，画家）。我们将此邻居集称为。 我们的邻居编码器的目的是编码并输出一个向量作为的潜在表示。由于这是一个大小不同的编码集的问题，我们希望编码函数能够（1)对排列不变，(2）对邻居集的大小不敏感。 我们使用满足上述性质的下列函数： 其中是关系实体对的特征表示，是激活函数。在文章中，设置了，在上验证得到了最好的性能。 将编码到中，首先使用维数为d的嵌入层emb（可以使用现有的基于嵌入的模型进行预训练）来获得和的向量表示： Dropout应用于向量，以实现更好的泛化。然后，我们应用前馈层（feed-forward layer）编码这个元组中的交互。 其中是要学习的参数并且代表连接。 由于我们在实体的闭集上工作，我们通过考虑背景知识中的实体与出现在、或中的实体之间的交集来绘制这个数字。 请注意，、或中的所有三元组都从中删除。 上：NELL；下：Wikidata。 为了在训练期间启用批处理，我们手动指定最大邻居数，并使用全零向量作为“虚拟”邻居。 虽然不同的实体有不同的程度（邻居的数量），但程度分布通常非常集中，如图2所示。 我们可以很容易地找到一个适当的界，作为批处理实体组的最大邻居数。 在这里提出的邻居编码器模块类似于关系图卷积网络(Schlichtkrull等人，2017年)，因为我们还使用共享内核{WC，BC}对不同实体的邻居进行编码。但与它们在整个图上工作并执行多个信息传播步骤的模型不同，我们只对实体的局部图进行编码并执行一步传播。这使我们能够轻松地将我们的模型应用于大规模的KGS，如Wikidata。此外，它们的模型也不适用于预先训练的图形嵌入。我们留下对其他图形编码策略的研究，例如。(Xu等人，2018年；Song等人，2018年)，给今后的工作。 Matching Processor 给定邻居编码器模块，现在我们讨论如何基于递归匹配处理器进行有效的相似性匹配。通过将应用于引用实体对和任何查询实体对，我们获得了每个实体对的邻居向量：。为了得到一个相似分数，可以用来在其他候选中进行排序，，我们可以简单地将每个对中的和连接起来，形成一个单对表示向量，并计算对之间的余弦相似度。 然而，这个简单的度量模型结果太浅，没有给出良好的性能。为了扩大我们的模型的容量，我们利用一个基于LSTM的(Hochreiter和Schmidhuber，1997)递归“处理”块(Vinyals等人，2015，2016)来执行多步匹配。每个流程步骤定义如下： 是一个具有输入，隐藏状态和单元状态的标准LSTM单元，并且是引用对和查询对的级联邻居向量。在个处理步骤之后，我们使用作为查询和支持实体对之间的最终相似性评分。每一个查询，通过比较，和，，我们可以得到每个的排名分数。 Loss Function and Training 对于查询关系及其引用/训练三元组，，，我们收集了一组正（真）查询三元组，，，，，并通过污染尾部实体构造了另一组负（假）查询三元组，，，，，根据以前基于嵌入的模型，我们使用铰链损失函数来优化我们的模型： 其中，和通过使用我们的度量模型将查询三元组，，与参考三元组，，进行比较来计算的标量，并且margin γ是要调整的超参数。对于每个训练集，我们首先从元训练集中采样一个任务/相关。然后，从中的所有已知三元组中，我们抽样了一个三元组作为参考/训练三元组，并抽样了一批其他三元组作为正向查询/测试三元组。训练过程的细节如图3算法1所示。 实验 数据集 4.数据集的统计 其中，#Ent表示唯一实体的数目，#R表示所有关系的数目，#Tasks表示我们作为一次性任务使用的关系数。 现有的知识图完成基准，如FB15k-237 (Toutanova et al.,2015) and YAGO3-10 (Mahdisoltani et al., 2013)都是真实世界KGS的小子集。这些数据集在训练和测试期间考虑了相同的关系集，并且通常为每个关系包含足够的训练三元组。为了构建一次学习的数据集，我们回到原始的KGS，并选择那些没有太多三元组的关系作为一次任务关系。 我们将其余的关系称为背景关系，因为它们的三元组为我们匹配实体对提供了重要的背景知识。 第一个数据集是基于 NELL (Mitchell et al., 2018)，这是一个通过浏览网页不断收集结构化知识的系统。我们采取最新的转储(dump)和删除这些反关系。我们选择小于500但超过50三元组的关系作为一次任务。为了证明我们的模型能够在大规模的KGS上操作，我们遵循类似的过程来构建另一个基于Wikidata的更大的数据集(Vrandeˇci´c and Kr¨otzsch, 2014). 数据集统计如表所示.请注意，Wiki-One数据集在实体和三元组的数量上比任何其他基准数据集大一个数量级。对于NELL-One，我们使用51/5/11任务关系进行培训/验证/测试。对于WikiOne，除法比是133：16：34。 实现细节 在我们的实验中，我们考虑了以下基于嵌入的方法：RESCAL(Nickel等人，2011年)、TransE(Bordes等人，2013年)、DistMult(Yang等人，2014年)和Complex(Trouillon等人，2016年)。对于TransE，我们使用Lin等人发布的代码(2015年b)。对于其他模型，我们已经尝试了Trouillon等人发布的代码(2016)。但在我们的数据集上，它给出的结果比TransE差得多。因此，我们使用了基于PyTorch的实现作为比较(Paszke等人，2017年)。在评估现有的嵌入模型时，在训练过程中，我们不仅使用背景关系的三元组，而且使用训练关系的所有三元组和这些Validation/test系的一次训练三元组。 然而，由于所提出的度量模型不需要嵌入查询关系，所以我们只包括嵌入训练的背景关系的三元组。由于TransE和DistMult使用一维向量来表示实体和关系，它们可以直接用于我们的模型。对于RESCAL，由于它使用矩阵来表示关系，所以我们在这些矩阵上使用平均池来获得一维嵌入。对于COMPLEX模型，我们使用实部和虚部的级联。我们模型的超参数在验证任务集上进行了调优，可以在附录中找到。 除了上述嵌入模型外，最近的一种方法(Dettmers等人，2017年)将卷积应用于模型关系，并在几个基准上获得最佳性能。对于每个查询，他们的模型列举了整个实体集，以获得正三元组和负三元组进行训练。我们发现，这种训练范式在处理大型实体集时需要大量的计算资源，不能扩展到具有数百万实体的Wikidata等现实世界的KGs。对于可伸缩性问题，我们的实验只考虑使用负抽样进行训练的模型。 结果 我们的方法的主要结果如上图所示。我们将我们的方法表示为“GMatching”，因为我们的模型被训练成匹配局部图形模式。我们使用平均倒数秩(MRR)和Hits@K来评估不同的模型。我们可以看到，我们的方法在这些一次关系上的各种嵌入模型上产生了一致的改进。在更大的Wiki-One数据集上，这些改进甚至更有实质性。为了研究模型的学习能力，我们还尝试用随机初始化的嵌入来训练我们的度量模型。令人惊讶的是，虽然结果比具有预先训练嵌入的度量模型差，但它们仍然优于基线嵌入模型。这表明，通过将邻居实体纳入我们的模型，许多关系和实体的嵌入实际上得到了有效的更新，并为我们的模型对测试数据进行预测提供了有用的信息。 值得注意的是，一旦经过训练，我们的模型可以用来预测任何新添加的关系，而不需要微调，而现有的模型通常需要重新训练来处理那些新添加的符号。在一个大的现实世界KG上，这个再训练过程可能是缓慢的，并且计算成本很高。 Remark on Model Selection 鉴于各种KG嵌入模型的存在，一个有趣的实验是将模型选择纳入超参数调优，并选择最佳的验证模型进行测试。如果我们考虑将KG嵌入和度量学习作为两种方法进行比较，那么模型选择过程的结果就可以作为比较的“最终”度量。例如，基线KG嵌入在Wiki-One上以RESCAL（11.9%）获得了最佳的MRR，因此我们报告了相应的测试MRR（7.2%）作为KG嵌入方法的最终模型选择结果。这样，在图5的上半部分，我们根据验证性能选择最佳的KG嵌入方法。重点突出了结果。同样，我们在底部选择最佳度量学习方法。从这个角度来看，我们的基于度量的方法也比KG嵌入有很大的优势。以MRR为例，所选择的度量模型在NELL-One上达到17.1%，在Wiki-One上达到20.0%，而KG嵌入的结果为9.3%和7.2%。改善分别为7.8%和12.8。 Analysis on Neighbor-Encoder 当我们的模型通过编码邻居来利用实体的局部图结构时，这里我们试图通过限制邻居的最大数量来研究邻居集的影响。 如果真邻居集的大小大于最大限制，则通过随机抽样选择邻居。图7上图显示了不同设置的学习曲线。 这些曲线是基于验证集上计算的Hits@10。对于每个实体来说，通常都会导致更好的性能。 我们还观察到，以最大值编码40个邻居的模型实际上比只编码30个邻居的模型产生更差的性能。 我们认为潜在的原因是，对于某些实体对，存在一些不相关的局部连接，并向模型提供噪声信息。我们看到对于每个实体来说，编码更多的邻居通常都会导致更好的性能。我们还观察到，以最大值编码40个邻居的模型实际上比只编码30个邻居的模型产生更差的性能。我们认为潜在的原因是，对于某些实体对，存在一些不相关的局部连接，并向模型提供噪声信息。 Ablation Studies 我们使用在NELL-One数据集上实现最佳Hits@10的模型进行消融研究。结果见图7下图的表。我们在验证和测试集上使用Hits@10进行比较，因为超参数是使用这个评估度量来选择的。 我们可以看到，匹配处理器和邻居编码器在我们的模型中都扮演着重要的角色。 另一个重要的观察是，缩放因子对邻居编码器非常重要。没有缩放，邻居编码器实际上比简单的基于嵌入的匹配给出更差的结果。 Performance on Different Relations 在测试各种模型时，我们观察到不同关系的结果实际上具有很高的方差。图6显示了由我们的最佳度量模型(GMatching-ComplEx)生成的NELL-One的分解结果及其相应的嵌入方法。作为参考，我们还报告了嵌入模型在标准训练设置下的性能，其中75%的三元组（而不是只有一个）用于训练，其余的用于测试。我们可以看到，与较小的候选集的关系通常更容易，我们的模型甚至可以比在标准设置下训练的嵌入模型表现得更好。对于一些关系，如运动员伤害他的身体部分，他们所涉及的实体在KG中很少有联系。正如预期的那样，一次性学习这些关系是相当具有挑战性的。这些与许多（&gt;3000）候选人的关系对所有模型都是具有挑战性的。即使对于具有更多训练三元组的嵌入模型，在某些关系上的性能仍然非常有限。这说明知识图完成任务还远未解决。 结论 本文介绍了一种一次性关系学习框架，可用于预测KGs长尾关系的新事实。我们的模型利用实体的局部图结构，并学习一个可微度量来匹配实体对。与通常需要微调以适应新关系的现有方法相比，我们的训练模型可以直接用于预测任何看不见的关系，并且在一次射击设置中也获得了更好的性能。我们今后的工作可能会考虑纳入外部文本数据，并加强我们的模型，以便更好地利用少数镜头学习案例中的多个培训示例。"},{"title":"hexo 主题使用mathjax插件渲染LaTeX数学公式的问题","date":"2020-11-04T09:26:46.889Z","url":"/SmartBoyMB.github.io/2020/11/04/howtosetmathjax/","tags":[["hexo","/SmartBoyMB.github.io/tags/hexo/"],["blog","/SmartBoyMB.github.io/tags/blog/"],["mathjax","/SmartBoyMB.github.io/tags/mathjax/"]],"categories":[["学习","/SmartBoyMB.github.io/categories/%E5%AD%A6%E4%B9%A0/"]],"content":"使用hexo为主题配置mathjax渲染LaTex数学公式 hexo本身并不支持LaTex格式的数学公式的渲染，一般都是通过mathjax插件来实现的，我查阅了很多资料，大部分都是针对于NEXT主题的一个设置，或者安装hexo-math工具包，以及手动配置mathjax.js等策略，很多都是复制粘贴，甚至代码都有错误。但经过实际操作，忙活了两天后，我分享一下我实现的方法。 使用hexo-filter-mathjax代替hexo-math 其实直接使用hexo-math是可以直接对公式进行渲染的，hexo-math通过标签插件将KaTeX和MathJax嵌入到Hexo帖子/页面中，方程在Hexo（服务器端）中呈现，因此不需要浏览器端javascript库，应将其删除，CSS样式是默认包含的。具体方法参考 但是不可避免，语法上hexo-math对于KaTeX和MathJax是分别使用下面两种格式的。 这与我们常使用的LaTex语法不同，并且更加复杂。所以用hexo-filter-mathjax插件可以直接编写LaTex语句。关于hexo-filter-mathjax插件安装与设置的具体方法参照 解决Hexo与MathJax渲染冲突问题 添加MathJax后，出现了部分渲染失败的问题，尤其是对有下划线'_'的公式。这是由于Hexo默认使用hexo-renderer-marked引擎渲染MarkDown，再交给MathjaX渲染，hexo-renderer-marked会吧一些特殊的markdown符号转换成相应的html标签，比如在markdown语法中，下划线_代表斜体，会被转化为&lt;em&gt;标签,所以MathJax引擎在渲染数学公式的时候就会出错。 在这里有人使用了hexo-renderer-kramed引擎替代，但我没有做尝试，而是使用了hexo-renderer-pandoc替代，他可以将Pandoc的markdown转换为HTML。 首先要确保安装了Pandoc，因为该渲染器是基于Pandoc上运行的。安装包下载地址为 安装完后运行下述的两个命令卸载hexo-renderer-marked和安装hexo-renderer-pandoc 其他的配置请参考地址"},{"title":"知识图谱学习（二）","date":"2020-10-27T08:35:16.510Z","url":"/SmartBoyMB.github.io/2020/10/27/kglearning2/","tags":[["知识图谱","/SmartBoyMB.github.io/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"],["概念","/SmartBoyMB.github.io/tags/%E6%A6%82%E5%BF%B5/"]],"categories":[["学习","/SmartBoyMB.github.io/categories/%E5%AD%A6%E4%B9%A0/"]],"content":"知识表示 知识应用的难点在于知识推理，知识推理的难点在于知识表示。知识表示是基于知识的人工智能应用中的核心部分。 知识表示的五个主要角色： 知识表示是一种代理，基于对事物的表示，我们无需实践而是通过思考和推理就可以得到有关外部世界的结论。 知识表示是一组本体论约定的集合，说明我们以什么样的方式来思考世界。 知识表示是只能推理的组成部分；推理需要知识进行表示，但知识表示不是推理的全部。 知识表示是高效计算的媒介；通过对知识进行有效组织，支持高效的推理。 知识表示是人类表达的媒介；基于通用表示框架，方便人们表达和分享对世界的认识。 基于五个角色，将从三个层次介绍各种知识表示方法。 人工智能和知识工程中的经典知识表示理论：逻辑、语义网络、框架、脚本。 语义网中的知识表示方法。 知识图谱中的知识表示方法以及在大数据环境下面向具体应用的知识图谱的数值化知识表示方法。 经典知识表示理论 逻辑 逻辑本身根据复杂性从简单到复杂分为：命题逻辑、一阶谓词逻辑、高阶逻辑。 命题逻辑与谓词逻辑区别：谓词逻辑存在量词，可以通过量词实现对对象集合的描述。两种量词：全称量词、存在量词。 语义网络 语义网络是一个通过语义关系连接的概念网络，它将知识表示为相互连接的点和边的模式。节点表示实体、事件、值等，边表示对象之间的语义关系。也就是说，语义网其实是一种有向图表示的知识系统，节点代表概念，而边则表示这些概念之间的语义关系。语义网络中最基本的单元称为语义基元，可以用三元组表示 &lt;节点1，关系，节点2 &gt;。 语义网络中的关系类型： （1）实例关系（ISA）：体现的是“具体与抽象”的概念，含义是“是一个”，表示一个事物是另个一个事物的一个实例。（类似于类的实例化） （2）分类关系（AKO）:亦称泛化关系，体现的是“子类与超类”的概念，含义为“是一种”，表示一个事物是另一个事物的一种类型。（类似于类的继承） （3）成员关系：体现的是“个体与集体”的关系，含义为“是一员”。表示一个事物是另一个事物的成员。（类似于类的组合） （4）属性关系：指事物和其属性之间的关系。常用的属性关系有： Have：含义为“有”，表示一个节点具有另一个节点所描述的属性。 Can：含义为“能”、“会”，表示一个节点能做另一个节点的事情。 （5）聚合关系：亦称包含关系，指具有组织或结构特征的“部分与整体”之间的关系。 （6）时间关系：指不同时间在其发生时间方面的先后次序关系。（时间上） （7）位置关系：指不同事物在位置方面的关系。（空间上） （8）相近关系：指不同事物在形状、内容等方面相似或接近。 可以按照论元个数把关系分为一元关系、二元关系和多元关系。 语义网络与一阶谓词具有相同的表达能力，不同的是，它用最简单的一种统一形式描述所有知识，非常有利于计算机的存储和检索。 语义网络的缺点是，它仅用节点及其关系描述知识，推理过程不像谓词逻辑表示方法那样明了，需要针对不同关系做不同处理，推理方法还不完善。 框架 框架将所有事物进行抽象，并用来表示事物各方面的属性以及事物之间的类属关系。 框架定义了这些概念的实例应该或可能具备的属性，这些属性被称为槽。若存在一个实体，就需要对教师框架中的槽或部分槽进行值的填充。 在原始的框架定义中，槽可以是任何形式的信息，包括原子值或值的集合；对于非原子的槽，还可以由多个侧面（facet）对槽的描述进行补充，这样做的目的是更立体准确地描述事物的属性与关系。 框架引入了层级结构，并根据类别之间的所属与细化，框架中的属性集合存在着继承的性质。 框架以强大的结构式表达能力和接近人类思维过程的特性，被应用与多个领域装甲系统的构建以及通用知识的表达。但框架表示法也有不可避免的缺陷，由于真实世界的多样性和复杂性，许多实际情况与框架原型存在较大的差异，在框架设计中难免引入错误或冲突。另外因为框架结构的复杂性，一方面，不同系统之间的框架很难对齐，另一方面，也给从非结构文本中抽取信息填充框架增加难度。 脚本 脚本通过一系列的原子动作来表示事物的基本行为，按照时间顺序描述事物的发生，类似于电影剧本。脚本表示的知识有确定的时间或因果顺序，必须是前一个动作完成后才会触发下一个动作的开始。与框架相比，脚本是用来描述一个动态过程而非静态知识的的表示方法。 一个完整的脚本应该包括以下几个重要的组成部分： 进入条件：指出脚本所描述的时间可能发生的先决条件，即事件发生的前提条件。 角色：描述事件中可能出现的人物。 道具：描述事件中可能出现的相关物体，主要指任务角色在完成动作时使用的工具。 舞台：脚本中事件发生的空间。 场景：时间发生的序列 结局：给出剧本所描述的时间发生以后通常所产生的结果，对应着进入后续脚本的先决条件。 脚本的表示能力有限，部剧本对元素基本属性的描述能力，也难以描述多变的时间发展可能方向。但在非常狭小的领域内，脚本表示方法却可以比其他方法更细致地刻画步骤和时序关系。 综上，经典知识表示理论中的语义网络、框架和脚本都属于基于槽的表示方法，有所区别的是槽是否具有层次、时序、控制关系。 语义网中的知识表示方法 语义网表示方法 语义网（Semantic Web）与人工智能中提出的语义网络（Semantic Network）的概念有所不同。 传统万维网中HTML文档的表示与语义网概念下XML格式文档表示的对比。语义网中，标签不再仅仅是网页格式的标志，而是含有自身的语义。 语义网知识描述体系 目前，语义网知识表示体系主要包括如下三个层次： XML：全称可扩展标记语言。 RDF：全称资源描述框架。RDF可以看成XML的扩展或简化。 OWL：全称网络本体语言。OWL是本体语义表示语言，建立在RDF和RDF Schema的基础之上。 知识图谱中的知识表示方法 表示框架 通常情况下，一个知识本体主要涵盖以下几个方面的内容： 事物：客观世界中的实体或对象。 概念：具有相似本体特征的一类事物，也称类型。 属性：事物或概念具有的特征和特性等。 关系：概念与实体之间的关联关系。 函数：事物或概念之间进行转化的形式表达。 约束：某项断言成立的限制条件的形式化描述。 规则：依据某项断言得到逻辑推论的因果关系知识的形式化描述。 公理：永远为真的断言。 狭义的知识图谱可以看成是知识库的图结构表示。以下介绍的知识图谱局限在现存的具有图有图结构的三元组知识库上，泛指三元组知识组成的有向图结构。 知识图谱的知识表示绝不仅仅体现在以RDF为基础框架的三元组之上，还体现在实体、类别、属性、关系等多颗粒度、多层次语义单元的关联之中，它是一个知识系统，以一种统一的方式表示了知识定义（Schema）和知识实例（Instance）两个层次的知识。 知识图谱的数值化表示方法 符号的数值化表示 知识表示的一大重要目标就是进行语义计算。 显式表示的知识，难以获取更全面的知识特征，另一方面，目前的大多数语义计算任务都采用基于数值计算的统计机器学习方法，而作为知识载体的数据表示是机器学习中的基础工作，数据表示的好坏直接影响到整个机器学习系统的性能。 文本的数值化表示 词是知识表示的最基本的单元，词的语义由上下文决定。 这种上下文分布表示词义的方法，并将这种表示用于词义消歧等任务，这类方法在当是被称为词空间模型。 2006年后，神经网络构造词表示的方法可以更灵活地对上下文进行建模，这类方法开始逐渐成为了词分布表示的主流方法。 知识图谱的数值化表示 表示学习也是知识图谱研究的热点任务之一，它把知识图谱中的离散符号（实体、属性、关系、值等）用连续型数值进行表示，这种表示能够体现实体和关系的语义信息，可以高效地计算实体、关系及其之间的复杂语义关联。 知识图谱表示学习的主要方法有张量分解模型和基于能量函数的模型等，它们都有各自的学习方式。 基于张量分解的表示学习方法 张量分解以RESCAL系统为代表，核心思想是将整个知识图谱编码为一个三维张量,如果三元组存在于知识图谱中，则对应张量中的值为1，否则为0。将张量分解为核心张量和因子矩阵的乘积形式，其中核心张量中每个二维矩阵切片代表一种关系的语义，因子矩阵中每一列代表一个实体的向量。模型重构的结果中的每一个元素被看做对应三元组成立的概率，如果概率大于某个阈值时，该三元组即为推断出的正确三元组。张量分解在编码实体和关系的过程中综合了整个知识图谱的信息，它的主要缺点是需要优化张量中所有位置的值，包括0在内。。因此，当关系数目较多时，张量的维度很高，分解过程计算量较大。 基于能量函数的表示学习方法 基于能量函数模型的方法是当前知识图谱表示学习研究最为丰富的方法，该类方法可以克服上述张量分解方法在大规模知识图谱表示学习过程学习效率低的问题。对于三元组，定义基于三元组的能量函数。如，TransE模型中能量函数定义为，和分别表示实体和关系的向量表示。记知识图谱中三元组的集合为,表示由生成的负样本的集合（比如，随机替换或），学习的目标函数定义为 其中是分离正样本和负样本的边界值。该目标函数的原理是使正样本的能量比负样本的能量低，通过惩罚负样本的能量值完成学习过程。 在测试的过程中，每种关系都设置一个能量阈值，如果三元组的能量小于阈值，则是正确的，否则不正确。因此，推理过程即使计算三元组能量的过程，当三元组能量值小于阈值时，该三元组为推断出的新三元组。不同能量模型的区别在于能量函数的构造不同。"},{"title":"知识图谱学习（一）","date":"2020-10-26T03:04:42.933Z","url":"/SmartBoyMB.github.io/2020/10/26/KnowledgeGraphLearning1/","tags":[["知识图谱","/SmartBoyMB.github.io/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"],["概念","/SmartBoyMB.github.io/tags/%E6%A6%82%E5%BF%B5/"]],"categories":[["学习","/SmartBoyMB.github.io/categories/%E5%AD%A6%E4%B9%A0/"]],"content":"知识图谱学习 这里将介绍有关知识图谱学习的第一部分，主要是知识图谱的一个概述。 知识的定义 陈述性知识（描述性知识） 描述客观事物的性状等静态信息，主要分为事物、概念、命题三个层次。 事物：特定的事和物。 概念：对一类事物本质特性的反映。 （3）命题 对事物之间的陈述。分为非概括性命题和概括性命题。 非概括性命题：特定的事物之间的关系。 概括性命题：概念之间的普遍关系。 过程性知识（程序性知识） 描述问题如何求解等动态信息，分为规则和控制结构两种类型。 规则：描述事物的因果关系。 控制结构：描述问题的求解步骤。 知识图谱的定义 图是一种能有效表示数据之间结构的表达形式，因此，人们考虑把数据中蕴含的知识用图的结构进行形式化表示。数据的结构化并和以后的结构化数据进行关联，就构成了知识图谱。 知识和结构化数据的表示和存储 在计算机科学领域，对知识和结构化数据的表示和存储具有不同的技术路线，最典型的包括本体（Ontology）和数据库（Database）两类。 本体 在人工只能领域，本体是通过对象类型、属性类型以及关系类型对领域知识进行形式化描述的模型。本体强调抽象的概念表示，因此只对数据的定义进行了描述，而没有描述具体的实例数据。 数据库 数据库是计算机科学家为了电脑表示和存储计算机应用中需要的数据而设计开发的产品。不同类型的数据库（关系数据库、面向对象数据库、非关系型数据库等），一般用于存储数据，这些数据可以进行传递和交换。数据库对于数据的描述和数据本身的操作提供了不同的描述语言，因此我们认为，数据库系统对数据描述和数据记录的表示和存储采用了不同的机制。 知识图谱的数据描述 实际上，人工智能应用中不仅需要具体的知识实例数据，数据的描述和定义也非常关键，例如概念上下位知识（‘大熊猫’是一种‘熊科动物’）、属性之间的关系（‘子女’与‘父母’是逆关系）、属性的约束（一个‘人’的‘父母’只有‘2个’）等。 知识图谱用统一的形式对知识实例数据的定义和具体知识数据进行描述，即用三元组形式（二元关系）对知识系统进行资源描述和存储。例如：WIkidata中不仅用&lt;Max Planck, instance of ,human&gt;表达具体的实例数据，也用&lt;place of birth, value type constraint, geographical object&gt;等三元组对相关知识结构进行了描述。 实例数据只有在满足系统约定的‘框架’约束下运用才能体现为‘知识’，其中“框架”（Schema或元知识）就是对知识的描述和定义，知识框架和实例数据共同构成一个完整的知识系统。 知识图谱以结构化三元组的形式存储现实世界中的实体以及实体之间的关系，表示为$ g=(,R,S) \\(,其中\\)={e_1,e_2,,e_(||)}\\(表示实体集合，\\)R={r_1,r_2,,r_(|R|)}\\(表示关系集合，\\)SR$表示知识图谱中三元组的集合。三元组通常描述了一个特定领域中的事实，由头实体、尾实体和描述这两个实体之间的关系组成。 尽管目前大部分知识图谱都以三元组的形式表示各种类型的知识，但是实际上知识图谱的知识表示绝不仅仅体现在以二元关系为基础的三元组上，还体现在实体、类别、属性、关系等多颗粒度、多层次语义单元的关联之中， 它以一种统一的方式体现知识定义（Schema）和知识实例（Instance）两个层次共同构成的知识系统。 知识框架 知识图谱应该包含哪些知识目前还没有统一定论。从知识工程的角度，知识框架一般包含了三个层次的知识： 概念知识：给出了知识的最基本内容。 事实知识：建立了概念之间的联系。 规则知识：建立了事物之间的联系。 综上，知识图谱以丰富的语义表示能力和灵活的结构构件了计算机世界中表示认知世界和物理世界中信息和知识的有效载体，成为人工智能应用的重要基础设施。 知识图谱类型 知识的类型（不同划分） 根据知识的主客观性，可以把知识分为事实性（或客观性）知识和主观性知识。 事实性知识: 通常是指那些确定性的。不随状态的变化而改变的知识，例如：“中华人民共和国的首都是北京”。 主观性知识：通常指某个人或群体的情感信息，例如：“大部分人都觉得苹果手机太贵了”这句话包含了用户对于苹果手机的意见和态度，但是这一态度会随着评论者的变化而变化。 根据知识变化的性质，已有的知识可以分为静态知识和动态知识。 静态知识：不随时间、空间的变化而变化，例如“生日”。 动态知识：随时间、空间的变化而变化的知识，例如“美国总统”。事件是动态知识的重要组成部分。 除此之外，我们也可以吧只是分为领域知识、百科知识、场景知识、语言知识、常识知识等等。 知识图谱的划分 已有的知识图谱根据领域和用途大致分为语言知识图谱、语言认识知识图谱、常识知识图谱、领域知识图谱以及百科知识图谱等几个类别。 对几个代表性的知识图谱简单介绍： Cyc Cyc是一个通用的常识数据库。 始建于1984年，其目的是将上百万条知识编码为机器可处理的形式，并在其基础上实现知识推理等智能信息处理任务。 主要由人工构建，包含50万实体，接近3万个关系以及500万事实。 用一阶谓词对知识进行描述，表达能力受限。 包含用于推理的常识规则，并提供多种推理引擎，支持演绎推理和归纳推理，同时也提供扩展推理机制的模块。 WordNet 一个英文电子词典和本体，将英文单词按照单词语义组成一个大的概念网络。 1985年由Princeton大学公布的一个英文电子词典和本体。 词语被聚类成同义词集。 知网（HowNet） 一个语言认知知识库/常识知识库，一概念为中心，基于义原描述了概念与概念之间以及概念所具有的属性之间的关系，每一个概念可以由多种语言的词汇进行描述（主要为中文和英文）。 由董振东教授主持开发。 知网是一个知识系统，经过多年的发展，目前知网总共包含了800多个义原，11000个词语。 ConceptNet ConceptNet是一个开放的、多语言的知识图谱。 致力于帮助计算机理解人们日常使用的单词意义。 由大量概念以及描述它们之间关系的常识构成，支持多种语言，可以用自然语言处理多种人工智能应用。 知识图谱的生命周期 主要包括知识体系构建、知识获取、知识融合、知识存储、知识推理和知识应用等。 知识体系构建 知识体系构建，也称知识建模。语义网的核心是让计算机能够理解文档中的数据，以及数据和数据之间的语义关联关系，从而使得计算机可以更加自动化、智能化地处理这些信息。与以往涉及面较广，只介绍与知识图谱数据建模紧密相关的核心概念——资源描述框架（RDF），RDF的基本数据模型包括了三个对象类型： 资源：能够使用RDF表示的对象称之为资源，包括互联网上的实体、事件和概念等。 谓词：谓词主要描述资源本身的特征和资源之间的关系。每一个谓词可以定义元知识。 陈述：一条陈述包含三个部分，通常称之为RDF三元组&lt;主题（subject）,谓词（predicate），宾语（object）&gt;。主体是被描述的资源，谓词表示主体的属性，也可以表示主体与宾语之间的关系。 知识获取 知识获取的目标是从海量的文本数据中通过信息抽取的方式获取知识，其方法根据所处理数据源的不同而不同。 知识图谱中的主要数据来源有各种形式的结构化数据、半结构化数据和非结构化文本数据（纯文本）。 相比于结构化和半结构化数据、非结构化文本数据要丰富的多。因此需要进行文本信息抽取，包括如下基本任务。 实体识别：实体识别任务的目标是从文本中识别实体信息。 实体消歧：实体消歧任务的目标是消除指定实体的歧义。 关系抽取：关系抽取的目标是获得两个实体之间的语义关系（可以是一元，二元，甚至更高元的关系）。 事件抽取：实践任务的目标是从描述事件信息的文本中抽取出用户感兴趣的事件信息并以结构化的的形式呈现出来。 知识融合 从融合的对象上看，知识融合包括：知识体系的融合和实例的融合。 知识体系的融合就是两个或多个异构知识体系进行融合，相同类别、属性、关系进行映射。 实例融合是对于两个不同知识图谱中的实例（实体实例，关系实例）进行融合，包括不同体系下的实例、不同语言的实例。 核心是计算两个知识图谱中的两个节点或边之间的语义映射关系。 从融合知识图谱类型上看，知识融合又分为：竖直方向的融合和水平方向的融合。 竖直方向的融合：指融合高层通用本体与底层领域本体或实例数据。 水平方向的融合：指融合同层次的知识图谱，实现实例数据的互补。 知识存储 目前知识图谱大多是基于图的数据结构，它的存储方式主要有两种形式：RDF格式存储和图数据库（Graph Database）。 RDF格式存储：就是以三元组的形式存储数据。不过由于逐行文本的存储使得搜索效率低下，为了提升三元组的搜索效率，通常采用六重索引的方法。 图数据库：比RDF数据库更加通用，目前典型的开源图数据库是Neo4j，这种图数据库的优点是具有完善的图查询语言，支持大多数图挖掘算法，它的缺点是数据更新慢，大节点的处理开销大。为了解决上述问题，子图筛选，子图同构判定等技术是目前图数据库的研究热点。 知识推理 由于处理数据的不完备性，所构建的知识图谱中肯定存在知识缺失现象（包括实体缺失、关系缺失） 由于数据的稀疏性，我们也很难利用抽取或者融合的方法对缺失的知识进行补齐。因此，需要采用推理的手段发现已有知识中的隐含的知识。 目前知识推理的研究主要集中在针对知识图谱中缺失关系的不足，即挖掘两个实体之间隐含的语义关系。所采用的的方法分为两种： 基于传统逻辑规则的方法进行推理：研究热点在于如何自动学习推理规则，以及如何解决推理过程中的规则冲突问题。 基于表示学习的推理：即采用学习的方式，将传统推理过程转化为基于分布式表示的语义向量相似度计算任务，这类方法的优点是容错率高、可学习，缺点也显而易见，即不可解释，缺乏语义约束。 当然，知识推理不仅应用于知识图谱的补全，也可以直接应用于相关应用任务。 知识应用 知识搜索：基于知识图谱的搜索引擎，可以根据用户查询准确地返回答案。用户意图理解是智能搜索的核心步骤。 自动问答：利用知识图谱中的实体及其关系进行推理得到答案。 推荐：利用知识图谱中的实体（商品）的关系（类别）向用户推荐相关的产品。 决策支持：知识图谱能够吧领域内的复杂知识通过信息抽取、数据挖掘、语义匹配、语义计算、知识推理等过程精确地描绘出来，并且可以描述知识的演化过程和发展规律，从而为研究和决策提供准确、可追踪、可解释、可推理的知识数据。 知识图谱与深度学习 基于数值计算的深度学习方法与基于符号表示和匹配的方法需要融合。 以下列举一些进展： 词的表示学习：是基于深度学习的自然语言处理方法的基础步骤，其主要目标是把每个词符号表示为分布向量的形式（Distributional Representation），基于词的向量表示，句子、段落、篇章、对话等更大的语言单元就可以通过语义组合模型（如卷积神经网络、循环神经网络等深度学习模型）得到，进而进行文本分类、问答匹配、机器翻译、对话生成等任务。 知识图谱表示学习：是将知识图谱中符号表示的实体和关系投影到低维向量空间中，这种表示能够体现实体和关系的语义信息，可以高效地计算实体、关系及其之间的复杂语义关联。 神经符号机：是一种将神经网络与符号推理相结合的技术，近年来开始被用于自然语言处理领域。神经符号机是利用神经网络对函数演算和图灵机等传统计算模型进行建模。根据抽象层次的不同，可以将其分为两类。一类为抽象层次较高，每个推理步骤都是从设计好的操作集合里选择，再选择合适的操作数，通过依次操作，得到最终答案。另一类的做法抽象层次较低，代表工作如神经图灵机、可微分神经计算机等，它们从模拟计算机底层操作方式的角度抽象模型，设计出内存、CPU控制器等结构，并借鉴传统计算机操作系统的思路，从随机访存、空闲内存分配、访存时序等不同侧面进行建模，完善模型的能力。 "},{"title":"LateXLearning","date":"2020-10-11T15:59:56.000Z","url":"/SmartBoyMB.github.io/2020/10/11/LateXLearning/","tags":[["LaTeX","/SmartBoyMB.github.io/tags/LaTeX/"]],"categories":[["学习","/SmartBoyMB.github.io/categories/%E5%AD%A6%E4%B9%A0/"]],"content":"LaTex Learning Life 在这里我会记录一些在学习LaTeX工具过程中遇到的一些问题和坑。 ## 编写伪代码 编写伪代码需要导入包algorithm、algorithmic、algpseudocode，但在实践中发现导入包algorithm后不要再导入algorithmic，否则会因为冲突报错，报错形式为Missing inserted。"},{"title":"Hello my blog life","date":"2020-10-09T11:06:06.000Z","url":"/SmartBoyMB.github.io/2020/10/09/Hellpmyblog/","tags":[["hexo","/SmartBoyMB.github.io/tags/hexo/"],["blog","/SmartBoyMB.github.io/tags/blog/"]],"categories":[["学习","/SmartBoyMB.github.io/categories/%E5%AD%A6%E4%B9%A0/"]],"content":"start my blog life 从今天开始就开始编写自己blog来记录自己的学习经历啦！！！ ## 如何来搭建一个博客 首先我是基于github平台，使用Hexo工具来搭建我的个人博客的。在这里不详细的说明整个搭建的过程，网络上有非常多且详细的教程来指导博客的搭建，这篇博客主要是说明我搭建时的部分内容和遇到的难题。 这是我搭建时参考的视频教程： GitHub仓库的设置 在创建github仓库时，需要将仓库名命名为yourname.github.io，并且进入setting界面，下滑到GitHub Pages选项，设置source下的Branch为你博客部署的分支，并点击save保存。 GitHub测试访问博客时404问题 在设置好后，如果按照视频教程中通过仓库名作为网址（README文件内网址）测试访问博客网址，会出现页面404的问题。这个问题难为了我很久，查询了资料才发现这个问题出现在我们创建github账号以后，创建的github的博客域名和自己的账号不一样导致的。如果直接通过仓库名作为网址访问的话，博客域名和账号名字要一样，不然会404。但是，如果名字不一样，就需要通过setting界面下GitHub Pages选项下的site网址访问。 Hexo工具部署博客到GitHub参数设置问题 Hexo文档地址： 在部署时，_config.yml文件的一些参数设置 参数 描述 url setting界面下GitHub Pages选项下的site地址。 root /yourname.github.io/(两个斜杠不能去省略) deploy：type git（可以是其他方式) deploy：repo 仓库的git地址(也可以是其他方式的地址) deploy：branch 对应在setting界面，下滑到GitHub Pages选项，设置source下的Branch名称 主题的下载和设置 主题下载地址： 首先在该地址浏览找到自己喜欢的主题，并通过提供的github链接下载文件放置在Hexo工具创建的博客文件的themes文件夹下。下一步在_config.yml文件内改变themes的参数为新主题名。其他的设置参照给的thems内的README文件内的说明进行设置。"}]